{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>Content of this notebook was prepared by Basel Shbita (shbita@usc.edu) as part of the class <u>DSCI 558: Building Knowledge Graphs</u> during Fall 2020 at University of Southern California (USC).</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RLTK to perform ER (i.e., Task 1) and Blocking (i.e., Task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Record Linkage ToolKit ([RLTK](https://rltk.readthedocs.io/en/master/)) is a general-purpose open-source record linkage platform that allows users to build powerful Python programs that link records referring to the same underlying entity.\n",
    "\n",
    "This notebook introduces some applied examples using RLTK. You can also find additional examples and use-cases in the same link provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw03_tasks_1_2 import rltk, json, g_tokenizer, create_dataset, get_ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset analysis & RLTK components construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need define how a single entry would like for each type of record (for each dataset). Similar code is already set up for you in the file `hw03_tasks_1_2.py` (the file you will submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLTK IMDB Record\n",
    "class IMDBRecord(rltk.Record):\n",
    "    ''' Record entry class for each of our IMDB records '''\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = ''\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def id(self):\n",
    "        return self.raw_object['url']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def name_string(self):\n",
    "        return self.raw_object['name']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def name_tokens(self):\n",
    "        global g_tokenizer\n",
    "        return set(g_tokenizer.tokenize(self.name_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFIRecord(rltk.Record):\n",
    "    ''' Record entry class for each of our AFI records '''\n",
    "    def __init__(self, raw_object):\n",
    "        super().__init__(raw_object)\n",
    "        self.name = ''\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def id(self):\n",
    "        return self.raw_object['url']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def title_string(self):\n",
    "        return self.raw_object['title']\n",
    "\n",
    "    @rltk.cached_property\n",
    "    def title_tokens(self):\n",
    "        global g_tokenizer\n",
    "        return set(g_tokenizer.tokenize(self.title_string))\n",
    "    \n",
    "    @rltk.cached_property\n",
    "    def date_string(self):\n",
    "        return self.raw_object.get('release_date', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your json-lines files into RLTK using the `rltk.JsonLinesReader`. It is already implemented for you to use, we can just call `create_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_imdb = create_dataset('imdb.jl', IMDBRecord)\n",
    "ds_afi = create_dataset('afi.jl', AFIRecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can inspect a few entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_imdb.generate_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_afi.generate_dataframe().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field (Attribute) Similarity (i.e., Task 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 2 example functions for field (attribute) similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_string_similarity_1(r_imdb, r_afi):\n",
    "    ''' Example dummy similiary function '''\n",
    "    s1 = r_imdb.name_string[:3]\n",
    "    s2 = r_afi.title_string[:3]\n",
    "    \n",
    "    return rltk.jaro_winkler_similarity(s1, s2)\n",
    "    \n",
    "def name_string_similarity_2(r_imdb, r_afi):\n",
    "    ''' Example dummy similiary function '''\n",
    "    s1 = r_imdb.name_string\n",
    "    s2 = r_afi.title_string\n",
    "    \n",
    "    if s1 == s2:\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Linking (i.e., Task 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how you can combine multiple similarity functions into a single weightened scoring function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold value to determine if we are confident the record match\n",
    "MY_TRESH = 0.8 # this number is just an example, you need to change it\n",
    "\n",
    "# entity linkage scoring function\n",
    "def rule_based_method(r_imdb, r_afi):\n",
    "    score_1 = name_string_similarity_1(r_imdb, r_afi)\n",
    "    score_2 = name_string_similarity_2(r_imdb, r_afi)\n",
    "    \n",
    "    total = 0.7 * score_1 + 0.3 * score_2\n",
    "    \n",
    "    # return two values: boolean if they match or not, float to determine confidence\n",
    "    return total > MY_TRESH, total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EL Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation is a built-in module for benchmarking. Lets load our development set and build a ground truth. We already implemented the loader, so lets just call it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load development set data\n",
    "gt = get_ground_truth('imdb_afi_el.dev.json', ds_imdb, ds_afi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate additional negatives for our ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.generate_all_negatives(ds_imdb, ds_afi, range_in_gt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run some candidates using the ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = rltk.Trial(gt)\n",
    "candidate_pairs = rltk.get_record_pairs(ds_imdb, ds_afi, ground_truth=gt)\n",
    "for r_imdb, r_afi in candidate_pairs:\n",
    "    result, confidence = rule_based_method(r_imdb, r_afi)\n",
    "    trial.add_result(r_imdb, r_afi, result, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate our trial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.evaluate()\n",
    "print('Trial statistics based on Ground-Truth from development set data:')\n",
    "print(f'tp: {trial.true_positives:.06f} [{len(trial.true_positives_list)}]')\n",
    "print(f'fp: {trial.false_positives:.06f} [{len(trial.false_positives_list)}]')\n",
    "print(f'tn: {trial.true_negatives:.06f} [{len(trial.true_negatives_list)}]')\n",
    "print(f'fn: {trial.false_negatives:.06f} [{len(trial.false_negatives_list)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using RDFLib for Knowledge Representation (i.e. Task 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDFLib is a Python library for working with RDF, a simple yet powerful language for representing information as graphs. RDFLib aims to be a pythonic RDF API, a Graph is a python collection of RDF Subject, Predicate,  Object Triples.\n",
    "\n",
    "This notebook introduces simple examples. You can also find additional information in the [official documenation](https://rdflib.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, URIRef, Literal, XSD, Namespace, RDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some namespaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOAF = Namespace('http://xmlns.com/foaf/0.1/')\n",
    "MYNS = Namespace('http://dsci558.org/myfakenamespace#')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a graph and bind the namespaces we defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kg = Graph()\n",
    "my_kg.bind('myns', MYNS)\n",
    "my_kg.bind('foaf', FOAF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a URI, then add a simple triple to the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_uri = URIRef(MYNS['dsci558_production_company'])\n",
    "my_kg.add((node_uri, RDF.type, MYNS['productionCompany']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triple that will be generated is: `myns:dsci558_production_company rdf:type myns:productionCompany` where `myns` is the prefix `http://dsci558.org/myfakenamespace#`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an additional triple (which describes the same subject, `node_uri`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kg.add((node_uri, FOAF['name'], Literal('DSCI 558 Production Company')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triple that will be generated is: `myns:dsci558_production_company foaf:name \"DSCI 558 Production Company\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's dump our graph triples into some `ttl` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kg.serialize('sample_graph.ttl', format=\"turtle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `sample_graph.ttl` and inspect it to see how your two triples look in `ttl` syntax."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
