{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `AmpliGraph` to generate GoT Knowledge Graph Embeddings\n",
    "\n",
    "<sub>Content of this notebook was prepared by Basel Shbita (shbita@usc.edu) as part of the class <u>DSCI 558: Building Knowledge Graphs</u> during Fall 2020 at University of Southern California (USC).</sub>\n",
    "\n",
    "**Notes**: \n",
    "- You are supposed to write your code or modify our code in any cell starting with `# ** STUDENT CODE`.\n",
    "- Much content of this notebook was borrowed from AmpliGraph tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AmpliGraph` is a suite of neural machine learning models for relational learning, a branch of machine learning that deals with supervised learning on knowledge graphs. It can be used to <u>generate stand-alone knowledge graph embeddings</u>, discover new knowledge from an existing knowledge graph and complete large knowledge graphs with missing statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this task, you will gain some hands-on experience working with Knowledge Graph Embeddings. Specifically, you will use the *TransE*, *DistMult* and *ComplEx* models to learn the embeddings of a (small) KG. You will be required to split the dataset to train and test sets, train the model, evaluate it and then generate a visualization for each model type!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.3.2'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ampligraph\n",
    "\n",
    "ampligraph.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "We will use the Game of Thrones (reduced) Knowledge Graph found in file `GoT.csv`.<br />\n",
    "Each relation (i.e. a triple) is in the form:`<subject, predicate, object>`\n",
    "\n",
    "Run the following cell to load the dataset in memory with using the `load_from_csv()` utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import load_from_csv\n",
    "\n",
    "X = load_from_csv('.', 'GoT.csv', sep=',') # numpy.ndarray; size (3175,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                              s            p  \\\n",
       "0                     Smithyton      SEAT_OF   \n",
       "1  House Mormont of Bear Island       LED_BY   \n",
       "2               Margaery Tyrell       SPOUSE   \n",
       "3         Maron Nymeros Martell  ALLIED_WITH   \n",
       "4  House Gargalen of Salt Shore    IN_REGION   \n",
       "\n",
       "                                   o  \n",
       "0         House Shermer of Smithyton  \n",
       "1                      Maege Mormont  \n",
       "2                  Joffrey Baratheon  \n",
       "3  House Nymeros Martell of Sunspear  \n",
       "4                              Dorne  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>s</th>\n      <th>p</th>\n      <th>o</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Smithyton</td>\n      <td>SEAT_OF</td>\n      <td>House Shermer of Smithyton</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>House Mormont of Bear Island</td>\n      <td>LED_BY</td>\n      <td>Maege Mormont</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Margaery Tyrell</td>\n      <td>SPOUSE</td>\n      <td>Joffrey Baratheon</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maron Nymeros Martell</td>\n      <td>ALLIED_WITH</td>\n      <td>House Nymeros Martell of Sunspear</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>House Gargalen of Salt Shore</td>\n      <td>IN_REGION</td>\n      <td>Dorne</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# inspect the top triples:\n",
    "pd.DataFrame(X, columns=['s', 'p', 'o']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list the subject and object entities found in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['Abelar Hightower', 'Acorn Hall', 'Addam Frey', ..., 'the Antlers',\n",
       "       'the Paps', 'unnamed tower'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# an array of unique subjects and objects => size (2050,)\n",
    "entities = np.unique(np.concatenate([X[:, 0], X[:, 2]])) \n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and all of the relationships that link them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['ALLIED_WITH', 'BRANCH_OF', 'FOUNDED_BY', 'HEIR_TO', 'IN_REGION',\n",
       "       'LED_BY', 'PARENT_OF', 'SEAT_OF', 'SPOUSE', 'SWORN_TO'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    " # an array of unique preicates => size (10,)\n",
    "relations = np.unique(X[:, 1])\n",
    "relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining train and test datasets\n",
    "\n",
    "As is typical in machine learning, we need to split our dataset into training and test sets.\n",
    "\n",
    "What differs from the standard method of randomly sampling N points to make up our test set, is that our data points are two entities linked by some relationship, and we need to take care to <u>ensure that all entities are represented in train and test sets by at least one triple</u>.\n",
    "\n",
    "To accomplish this, `AmpliGraph` provides the `train_test_split_no_unseen` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen \n",
    "\n",
    "# we create a 10% test set split\n",
    "X_train, X_test = train_test_split_no_unseen(X, test_size=int(X.shape[0]/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now split into train/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train set size:  (2858, 3)\nTest set size:  (317, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1\n",
    "## Task 2.1.x.1 Training the model\n",
    "\n",
    "`AmpliGraph` has implemented several Knoweldge Graph Embedding models (*TransE, ComplEx, DistMult, etc...*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features import TransE, DistMult, ComplEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go through the parameters to understand what's going on:\n",
    "- **k**: the dimensionality of the embedding space\n",
    "- **eta ($\\eta$)**: the number of negative, or false triples that must be generated at training runtime for each positive, or true triple\n",
    "- **batches_count**: the number of batches in which the training set is split during the training loop. If you are having into low memory issues than settings this to a higher number may help.\n",
    "- **epochs**: the number of epochs to train the model for.\n",
    "- **optimizer**: the Adam optimizer, with a learning rate of 1e-3 set via the *optimizer_params* kwarg.\n",
    "- **loss**: pairwise loss, with a margin of 0.5 set via the *loss_params* kwarg.\n",
    "- **regularizer**: $L_p$ regularization with $p=2$, i.e. l2 regularization. $\\lambda$ = 1e-5, set via the *regularizer_params* kwarg.\n",
    "\n",
    "Now we can instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE \n",
    "# TODO: try different model types: TransE [2.1.1], DistMult [2.1.2], ComplEx [2.1.3] \n",
    "EmbeddingMethod = ComplEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingMethod(batches_count=100, \n",
    "                seed=0, \n",
    "                epochs=200, \n",
    "                k=150, \n",
    "                eta=5,\n",
    "                optimizer='adam', \n",
    "                optimizer_params={'lr':1e-3},\n",
    "                loss='multiclass_nll', \n",
    "                regularizer='LP', \n",
    "                regularizer_params={'p':3, 'lambda':1e-5}, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering negatives\n",
    "\n",
    "`AmpliGraph` aims to follow `scikit-learn`'s ease-of-use design philosophy and simplify everything down to `fit`, `evaluate`, and `predict` functions.\n",
    "\n",
    "However, there are some knowledge graph specific steps we must take to ensure our model can be trained and evaluated correctly. The first of these is defining the filter that will be used to ensure that no *negative* statements generated by the corruption procedure are actually positives. This is simply done by concatenating our train and test sets. Now when negative triples are generated by the corruption strategy, we can check that they aren't actually true statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_filter = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Once you run the next cell the model will train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Average Loss:   0.016231: 100%|██████████| 200/200 [05:55<00:00,  1.78s/epoch]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X_train, early_stopping = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.x.2 Evaluating the model\n",
    "\n",
    "Now it's time to evaluate our model on the test set to see how well it's performing.\n",
    "\n",
    "For this we'll use the `evaluate_performance` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import evaluate_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the arguments to this function:\n",
    "\n",
    "- `X`: the data to evaluate on. We're going to use our test set to evaluate.\n",
    "- `model`: the model we previously trained.\n",
    "- `filter_triples`: will filter out the false negatives generated by the corruption strategy.\n",
    "- `use_default_protocol`: specifies whether to use the default corruption protocol. If True, then subj and obj are corrupted separately during evaluation.\n",
    "- `verbose`: will give some nice log statements. Let's leave it on for now.\n",
    "\n",
    "Let's run some evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n",
      "100%|██████████| 317/317 [00:06<00:00, 50.56it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,\n",
    "                             use_default_protocol=True,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ranks` returned by the `evaluate_performance` function <mark>indicate the rank at which the test set triple was found </mark> when performing link prediction using the model.\n",
    "\n",
    "<u>For example</u>, if we run the triple `<House Stark of Winterfell, IN_REGION, The North>` and the model returns a rank of `7`, it tells us that while it's not the highest likelihood true statement (which would be given a rank 1), it's pretty likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### metrics\n",
    "For the evaluation metrics, we are going to use the `mrr_score` (mean reciprocal rank) and `hits_at_n_score` functions:\n",
    "- `mrr_score`: The function computes the mean of the reciprocal of elements of a vector of rankings ranks.\n",
    "- `hits_at_n_score`: The function computes how many elements of a vector of rankings ranks make it to the top n positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MRR: 0.35\nHits@10: 0.47\nHits@3: 0.38\nHits@1: 0.29\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Hits@N](http://docs.ampligraph.org/en/1.0.3/generated/ampligraph.evaluation.hits_at_n_score.html#ampligraph.evaluation.hits_at_n_score) indicates how many times in average a true triple was ranked in the top-N. Therefore, on average, we guessed the correct subject or object 53% of the time when considering the top-3 better ranked triples. The choice of which N makes more sense depends on the application.\n",
    "\n",
    "The [Mean Reciprocal Rank (MRR)](http://docs.ampligraph.org/en/latest/generated/ampligraph.evaluation.mrr_score.html) is another popular metrics to assess the predictive power of a model.\n",
    "\n",
    "**^ Please note that a screenshot of these scores are required for task of 2.1.x.2 ^**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting New Links\n",
    "\n",
    "Link prediction allows us to infer missing links in a graph. This has many real-world use cases, such as predicting connections between people in a social network, interactions between proteins in a biological network, and music recommendation based on prior user taste.\n",
    "\n",
    "In our case, we are going to see which of the following candidate statements are more likely to be true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unseen = np.array([\n",
    "    ['Jorah Mormont', 'SPOUSE', 'Daenerys Targaryen'],\n",
    "    [\"King's Landing\", 'SEAT_OF', 'House Lannister of Casterly Rock'],\n",
    "    ['Brienne of Tarth', 'SPOUSE', 'Jaime Lannister'],\n",
    "    ['House Stark of Winterfell', 'IN_REGION', 'The North'],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 25.36it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks_unseen = evaluate_performance(\n",
    "    X_unseen, \n",
    "    model=model, \n",
    "    filter_triples=unseen_filter,\n",
    "    corrupt_side = 's+o',\n",
    "    use_default_protocol=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the scores (real numbers) into probabilities (bound between 0 and 1) using the `expit` transform (note that the probabilities are not calibrated).\n",
    "\n",
    ">Advanced note: To calibrate the probabilities, one may use a procedure such as [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling) or [Isotonic regression](https://en.wikipedia.org/wiki/Isotonic_regression). The challenge is to define what is a true triple and what is a false one, as the calibration of the probability of a triple being true depends on the base rate of positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "probs = expit(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           statement  rank     score      prob\n",
       "3      House Stark of Winterfell IN_REGION The North   177  1.308594  0.787278\n",
       "1  King's Landing SEAT_OF House Lannister of Cast...  1331  0.196228  0.548900\n",
       "0            Jorah Mormont SPOUSE Daenerys Targaryen  2232  0.039165  0.509790\n",
       "2            Brienne of Tarth SPOUSE Jaime Lannister  2430 -0.124812  0.468837"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>statement</th>\n      <th>rank</th>\n      <th>score</th>\n      <th>prob</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>House Stark of Winterfell IN_REGION The North</td>\n      <td>177</td>\n      <td>1.308594</td>\n      <td>0.787278</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>King's Landing SEAT_OF House Lannister of Cast...</td>\n      <td>1331</td>\n      <td>0.196228</td>\n      <td>0.548900</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Jorah Mormont SPOUSE Daenerys Targaryen</td>\n      <td>2232</td>\n      <td>0.039165</td>\n      <td>0.509790</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brienne of Tarth SPOUSE Jaime Lannister</td>\n      <td>2430</td>\n      <td>-0.124812</td>\n      <td>0.468837</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
    "                      ranks_unseen, \n",
    "                      np.squeeze(scores),\n",
    "                      np.squeeze(probs))), \n",
    "             columns=['statement', 'rank', 'score', 'prob']).sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.x.3: Visualizing Embeddings with Tensorboard projector\n",
    "\n",
    "we can now visualize the high-dimensional embeddings in the browser. Lets import the `create_tensorboard_visualization` function, which simplifies the creation of the files necessary for Tensorboard to display the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll run the function with our model, specifying the output path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, 'dsci558_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, we should now have a number of files in the `AmpliGraph/tutorials/GoT_embeddings` directory:\n",
    "\n",
    "```\n",
    "GoT_embeddings/\n",
    "    ├── checkpoint\n",
    "    ├── embeddings_projector.tsv\n",
    "    ├── graph_embedding.ckpt.data-00000-of-00001\n",
    "    ├── graph_embedding.ckpt.index\n",
    "    ├── graph_embedding.ckpt.meta\n",
    "    ├── metadata.tsv\n",
    "    └── projector_config.pbtxt\n",
    "```\n",
    "\n",
    "To visualize the embeddings in Tensorboard, run the following from your command line:\n",
    "\n",
    "```bash\n",
    "tensorboard  --logdir=\"./dsci558_embeddings\"\n",
    "```\n",
    "    \n",
    ".. and once your browser opens up you should be able to see and explore your embeddings as below (PCA-reduced, two components):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^ Please note that a screenshot of embedding visualization is required for task 2.1.x.3 ^**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('ampligraph': conda)",
   "display_name": "Python 3.7.9 64-bit ('ampligraph': conda)",
   "metadata": {
    "interpreter": {
     "hash": "7bbd3b9324898850aa8841c0d1250fc1efcc5a9f3bbd89fed668d6d1378ae428"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}